# ========================================
# RAG SYSTEM CLOUD CONFIGURATION
# For Vercel/Cloud Deployment
# ========================================

# ----------------------------------------
# MongoDB Atlas (Database)
# ----------------------------------------
# Get from: https://cloud.mongodb.com
# 1. Create free cluster
# 2. Get connection string from "Connect" → "Connect your application"
MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/
MONGODB_DATABASE=autoassist
MONGODB_COLLECTION=cars_new

# ----------------------------------------
# Qdrant Cloud (Vector Database)
# ----------------------------------------
# Get from: https://cloud.qdrant.io (Free tier available)
# 1. Sign up for free account
# 2. Create a cluster
# 3. Get URL and API key from cluster settings
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your-qdrant-api-key
QDRANT_COLLECTION_NAME=cars_rag

# ----------------------------------------
# Embedding Model
# ----------------------------------------
# Using sentence-transformers (runs in backend)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Number of documents to retrieve
RETRIEVAL_K=5

# ----------------------------------------
# LLM Configuration (Choose ONE)
# ----------------------------------------

# Option 1: Groq API (⚡ FASTEST & RECOMMENDED for production)
# Get from: https://console.groq.com/keys
# Free tier: 30 requests/min, 7,000 requests/day
# Speed: 500+ tokens/sec (10x faster than alternatives)
GROQ_API_KEY=your-groq-api-key
GROQ_MODEL=llama-3.3-70b-versatile

# Option 2: Google Gemini
# Get from: https://makersuite.google.com/app/apikey
# Free tier: 60 requests per minute
# GEMINI_API_KEY=your-gemini-api-key
# GEMINI_MODEL=gemini-pro

# Option 3: HuggingFace Inference API
# Get from: https://huggingface.co/settings/tokens
# Note: Free tier has rate limits and model availability issues
# HF_API_KEY=your-huggingface-token
# HF_MODEL_ENDPOINT=https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2

# Option 3: Ollama (LOCAL ONLY - NOT for Vercel)
# OLLAMA_URL=http://localhost:11434
# OLLAMA_MODEL=llama2

# ----------------------------------------
# LLM Parameters
# ----------------------------------------
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9

# ----------------------------------------
# Data Source Configuration
# ----------------------------------------
# Set to "true" to use MongoDB, "false" to use local JSON files
USE_MONGODB=true

# ----------------------------------------
# FastAPI Backend (for development)
# ----------------------------------------
# This will be deployed separately or on a serverless function
RAG_API_URL=http://localhost:8000

