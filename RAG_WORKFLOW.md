# ğŸ”„ RAG Pipeline & Backend Workflow

Complete workflow documentation for the Retrieval-Augmented Generation (RAG) system for car recommendations.

---

## ğŸ“‹ Table of Contents

1. [System Architecture Overview](#system-architecture-overview)
2. [Initialization Workflow](#initialization-workflow)
3. [Real-Time Query Processing Workflow](#real-time-query-processing-workflow)
4. [Component Interactions](#component-interactions)
5. [Data Flow Diagrams](#data-flow-diagrams)
6. [API Endpoints & Request Flow](#api-endpoints--request-flow)
7. [Error Handling & Fallbacks](#error-handling--fallbacks)

---

## ğŸ—ï¸ System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Next.js App (React/TypeScript)                          â”‚   â”‚
â”‚  â”‚  - AI Chat Interface Component                           â”‚   â”‚
â”‚  â”‚  - RAG Recommendation Panel                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ HTTP POST /api/ai/rag-chat
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      NEXT.JS API LAYER                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  /api/ai/rag-chat/route.ts                              â”‚   â”‚
â”‚  â”‚  - Request validation                                    â”‚   â”‚
â”‚  â”‚  - Session management                                    â”‚   â”‚
â”‚  â”‚  - Proxy to FastAPI backend                             â”‚   â”‚
â”‚  â”‚  - Response enrichment (fetch full car data)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ HTTP POST http://localhost:8000/chat
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASTAPI BACKEND LAYER                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  main.py - /chat endpoint                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Step 1: Query Understanding (query_understanding) â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Analyze query with LLM (Gemini/Groq)           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Extract filters & search keywords               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Combine with chat history context              â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Step 2: RAG Chain (chain.py)                     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Create/retrieve ConversationalRetrievalChain    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Query with optimized search keywords            â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Step 3: Response Refinement (refiner.py)         â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Enhance RAG response with LLM                   â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  - Add general automotive knowledge                â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG CHAIN COMPONENTS    â”‚    â”‚   VECTOR DATABASE        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  retriever.py      â”‚  â”‚    â”‚  â”‚  Qdrant Vector DB  â”‚ â”‚
â”‚  â”‚  - CustomQdrant   â”‚â—„â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”¤  â”‚  - 384-dim vectors â”‚ â”‚
â”‚  â”‚    Retriever      â”‚  â”‚    â”‚  â”‚  - Metadata filters â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â”‚  - Payload indexes â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚  model.py          â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚  - LLM wrapper     â”‚  â”‚
â”‚  â”‚  (Groq/Gemini)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  embed.py          â”‚  â”‚
â”‚  â”‚  - Embeddings      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA SOURCE LAYER       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MongoDB Atlas    â”‚  â”‚
â”‚  â”‚  - Car database    â”‚  â”‚
â”‚  â”‚  - Full specs     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Initialization Workflow

### Phase 1: Data Preparation & Embedding

**Purpose**: One-time setup to create vector embeddings from car data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Data Loading                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: llm/backend/rag/mongodb_loader.py                     â”‚
â”‚                                                             â”‚
â”‚ 1. Connect to MongoDB Atlas                                 â”‚
â”‚    - Use MONGODB_URI from .env                             â”‚
â”‚    - Access collection: cars_new                            â”‚
â”‚                                                             â”‚
â”‚ 2. Fetch car documents                                      â”‚
â”‚    - Query all cars (or limit if specified)                â”‚
â”‚    - Transform MongoDB schema to RAG format                 â”‚
â”‚                                                             â”‚
â”‚ 3. Field Transformation:                                     â”‚
â”‚    "Identification Brand" â†’ "make"                          â”‚
â”‚    "Pricing Delhi Ex Showroom Price" â†’ "price_lakhs"        â”‚
â”‚    "Engine Type" â†’ "fuel_type"                              â”‚
â”‚    ... (all fields normalized)                              â”‚
â”‚                                                             â”‚
â”‚ 4. Generate Rich Descriptions                                â”‚
â”‚    - create_description_from_record()                       â”‚
â”‚    - Natural language text combining:                        â”‚
â”‚      â€¢ Make, Model, Variant, Year                           â”‚
â”‚      â€¢ Body type, Segment                                   â”‚
â”‚      â€¢ Price in â‚¹ lakhs                                     â”‚
â”‚      â€¢ Engine specs (power, torque, displacement)            â”‚
â”‚      â€¢ Fuel efficiency (mileage)                            â”‚
â”‚      â€¢ Safety features (airbags, ABS, ESC)                  â”‚
â”‚      â€¢ Comfort features (sunroof, cruise control)           â”‚
â”‚      â€¢ Infotainment features                                â”‚
â”‚      â€¢ ADAS features                                        â”‚
â”‚      â€¢ Practicality (seating, boot space)                   â”‚
â”‚                                                             â”‚
â”‚ Output: List of car records with "description" field       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Embedding Generation                                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: llm/backend/rag/embed.py                              â”‚
â”‚                                                             â”‚
â”‚ 1. Load Embedding Model                                     â”‚
â”‚    - Model: sentence-transformers/all-MiniLM-L6-v2          â”‚
â”‚    - Vector size: 384 dimensions                            â”‚
â”‚    - Device: CPU (or GPU if available)                      â”‚
â”‚                                                             â”‚
â”‚ 2. Generate Embeddings                                      â”‚
â”‚    - Input: Car descriptions (text strings)                 â”‚
â”‚    - Process in batches (default: 50 cars/batch)            â”‚
â”‚    - Output: 384-dim vectors for each car                  â”‚
â”‚                                                             â”‚
â”‚ 3. Prepare Qdrant Points                                     â”‚
â”‚    For each car:                                            â”‚
â”‚    - Vector: 384-dim embedding                             â”‚
â”‚    - Payload: Complete car metadata (all fields)            â”‚
â”‚      â€¢ Includes: make, model, price, mileage, features...   â”‚
â”‚      â€¢ Special field: "page_content" = description text    â”‚
â”‚    - ID: Sequential index or unique identifier             â”‚
â”‚                                                             â”‚
â”‚ 4. Create Qdrant Collection                                  â”‚
â”‚    - Collection name: "cars_rag" (from env)                â”‚
â”‚    - Vector config:                                         â”‚
â”‚      â€¢ Size: 384                                            â”‚
â”‚      â€¢ Distance: Cosine similarity                          â”‚
â”‚    - Create payload indexes for filtering:                  â”‚
â”‚      â€¢ price_lakhs (FLOAT)                                  â”‚
â”‚      â€¢ body_type (KEYWORD)                                  â”‚
â”‚      â€¢ fuel_type (KEYWORD)                                  â”‚
â”‚      â€¢ segment (KEYWORD)                                   â”‚
â”‚      â€¢ mileage (FLOAT)                                      â”‚
â”‚      â€¢ year (INTEGER)                                       â”‚
â”‚      â€¢ transmission_type (KEYWORD)                          â”‚
â”‚      â€¢ power_bhp (FLOAT)                                    â”‚
â”‚      â€¢ airbags (INTEGER)                                    â”‚
â”‚      â€¢ make (KEYWORD)                                       â”‚
â”‚      â€¢ model (KEYWORD)                                      â”‚
â”‚                                                             â”‚
â”‚ 5. Upsert to Qdrant                                         â”‚
â”‚    - Batch upsert (50 points at a time)                     â”‚
â”‚    - Store vectors + metadata in Qdrant                      â”‚
â”‚                                                             â”‚
â”‚ Output: Qdrant collection with all car embeddings          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Command to Run**:
```bash
cd llm/backend
python -m rag.embed --mongodb
```

---

## ğŸ”„ Real-Time Query Processing Workflow

### Complete Request Flow

```
USER QUERY: "Show me fuel-efficient SUVs under 15 lakhs"
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Frontend Request                                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ Component: src/components/features/ai-chat-interface.tsx    â”‚
â”‚                                                             â”‚
â”‚ 1. User types query in chat input                           â”‚
â”‚ 2. On submit, calls: POST /api/ai/rag-chat                 â”‚
â”‚    Body: {                                                  â”‚
â”‚      query: "Show me fuel-efficient SUVs under 15 lakhs",   â”‚
â”‚      session_id: "session_123..." (or auto-generated)      â”‚
â”‚    }                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Next.js API Proxy                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: src/app/api/ai/rag-chat/route.ts                      â”‚
â”‚                                                             â”‚
â”‚ 1. Validate request                                         â”‚
â”‚    - Check query is non-empty string                        â”‚
â”‚                                                             â”‚
â”‚ 2. Generate/use session ID                                  â”‚
â”‚    - Use provided session_id or generate new one           â”‚
â”‚                                                             â”‚
â”‚ 3. Forward to FastAPI backend                               â”‚
â”‚    POST http://localhost:8000/chat                          â”‚
â”‚    Headers: Content-Type: application/json                  â”‚
â”‚    Body: {                                                  â”‚
â”‚      query: "...",                                          â”‚
â”‚      session_id: "...",                                     â”‚
â”‚      filters: null (optional)                              â”‚
â”‚    }                                                        â”‚
â”‚                                                             â”‚
â”‚ 4. Wait for FastAPI response                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: FastAPI Backend - Query Understanding             â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: llm/backend/main.py - /chat endpoint                â”‚
â”‚                                                             â”‚
â”‚ STEP 3.1: Load Chat History                                 â”‚
â”‚    - Retrieve session history from session_histories dict  â”‚
â”‚    - Get last 10 message pairs (Q, A)                       â”‚
â”‚                                                             â”‚
â”‚ STEP 3.2: Query Understanding (query_understanding.py)      â”‚
â”‚    Function: understand_query_with_llm()                   â”‚
â”‚                                                             â”‚
â”‚    a) Build prompt with:                                    â”‚
â”‚       - Current query                                        â”‚
â”‚       - Chat history (last 3 exchanges)                     â”‚
â”‚       - Instructions for intent extraction                   â”‚
â”‚                                                             â”‚
â”‚    b) Call LLM (Groq/Gemini):                              â”‚
â”‚       - Model: llama-3.3-70b-versatile (Groq)              â”‚
â”‚       - Temperature: 0.3 (structured output)                â”‚
â”‚                                                             â”‚
â”‚    c) Parse JSON response:                                  â”‚
â”‚       {                                                      â”‚
â”‚         "combined_intent": "User wants fuel-efficient...", â”‚
â”‚         "filters": {                                        â”‚
â”‚           "body_type": "SUV",                               â”‚
â”‚           "price_max": 15.0,                                â”‚
â”‚           "mileage_min": 18.0                               â”‚
â”‚         },                                                  â”‚
â”‚         "search_keywords": "fuel efficient SUV",            â”‚
â”‚         "context_notes": "..."                              â”‚
â”‚       }                                                     â”‚
â”‚                                                             â”‚
â”‚    Output:                                                  â”‚
â”‚    - auto_filters: Extracted filter dictionary              â”‚
â”‚    - search_keywords: Optimized query for semantic search  â”‚
â”‚    - combined_intent: Full understanding of user need      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: Filter Merging & Chain Creation                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: llm/backend/main.py                                  â”‚
â”‚                                                             â”‚
â”‚ STEP 4.1: Merge Filters                                      â”‚
â”‚    - Start with auto_filters from query understanding      â”‚
â”‚    - Merge with explicit filters (if provided)              â”‚
â”‚    - Explicit filters take precedence                       â”‚
â”‚    - Result: final_filters = {                              â”‚
â”‚        "body_type": "SUV",                                  â”‚
â”‚        "price_max": 15.0,                                  â”‚
â”‚        "mileage_min": 18.0                                 â”‚
â”‚      }                                                      â”‚
â”‚                                                             â”‚
â”‚ STEP 4.2: Get or Create Chain                               â”‚
â”‚    Function: get_or_create_chain()                         â”‚
â”‚                                                             â”‚
â”‚    a) Check if chain exists for session + filters           â”‚
â”‚       - Key: f"{session_id}_{hash(filters)}"               â”‚
â”‚                                                             â”‚
â”‚    b) If not exists, create new chain:                     â”‚
â”‚       Function: create_chain(filters, k=8)                 â”‚
â”‚                                                             â”‚
â”‚       i) Get LLM (model.py):                                â”‚
â”‚          - Try Groq first (fastest)                         â”‚
â”‚          - Fallback to Gemini                               â”‚
â”‚          - Fallback to Ollama (local dev)                   â”‚
â”‚                                                             â”‚
â”‚       ii) Get Retriever (retriever.py):                     â”‚
â”‚          Function: get_retriever(filters, k=8)             â”‚
â”‚                                                             â”‚
â”‚          â€¢ Initialize embeddings model                      â”‚
â”‚            (same as embedding: all-MiniLM-L6-v2)            â”‚
â”‚          â€¢ Get Qdrant client                                â”‚
â”‚          â€¢ Build Qdrant filter from filters dict            â”‚
â”‚          â€¢ Create CustomQdrantRetriever                     â”‚
â”‚                                                             â”‚
â”‚       iii) Load prompt template                             â”‚
â”‚          - File: prompts/base_prompt.txt                    â”‚
â”‚                                                             â”‚
â”‚       iv) Create ConversationalRetrievalChain               â”‚
â”‚          - Combines LLM + Retriever + Prompt                â”‚
â”‚          - Uses LCEL (LangChain Expression Language)         â”‚
â”‚                                                             â”‚
â”‚    c) Cache chain in session_chains dict                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: Retrieval Phase                                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: llm/backend/rag/retriever.py                         â”‚
â”‚ Class: CustomQdrantRetriever                               â”‚
â”‚                                                             â”‚
â”‚ STEP 5.1: Query Embedding                                   â”‚
â”‚    - Input: "fuel efficient SUV" (optimized keywords)      â”‚
â”‚    - Generate 384-dim vector using embedding model         â”‚
â”‚    - Same model used during initialization                  â”‚
â”‚                                                             â”‚
â”‚ STEP 5.2: Build Qdrant Filter                               â”‚
â”‚    Function: build_qdrant_filter(final_filters)             â”‚
â”‚                                                             â”‚
â”‚    Convert filters to Qdrant Filter object:                 â”‚
â”‚    Filter(                                                  â”‚
â”‚      must=[                                                 â”‚
â”‚        FieldCondition(                                      â”‚
â”‚          key="body_type",                                   â”‚
â”‚          match=MatchValue(value="SUV")                      â”‚
â”‚        ),                                                   â”‚
â”‚        FieldCondition(                                      â”‚
â”‚          key="price_lakhs",                                 â”‚
â”‚          range=Range(lte=15.0)                              â”‚
â”‚        ),                                                   â”‚
â”‚        FieldCondition(                                      â”‚
â”‚          key="mileage",                                     â”‚
â”‚          range=Range(gte=18.0)                              â”‚
â”‚        )                                                    â”‚
â”‚      ]                                                      â”‚
â”‚    )                                                        â”‚
â”‚                                                             â”‚
â”‚ STEP 5.3: Vector Similarity Search                          â”‚
â”‚    - Query Qdrant with:                                    â”‚
â”‚      â€¢ query: NearestQuery(nearest=query_embedding)        â”‚
â”‚      â€¢ limit: 8 (top K documents)                          â”‚
â”‚      â€¢ query_filter: Filter object (from step 5.2)         â”‚
â”‚      â€¢ with_payload: True (get all metadata)                â”‚
â”‚                                                             â”‚
â”‚    - Qdrant returns:                                        â”‚
â”‚      â€¢ Top 8 most similar cars                              â”‚
â”‚      â€¢ Each with:                                           â”‚
â”‚        - Similarity score                                   â”‚
â”‚        - Full payload (all car metadata)                    â”‚
â”‚                                                             â”‚
â”‚ STEP 5.4: Document Extraction                               â”‚
â”‚    - Convert Qdrant points to LangChain Documents          â”‚
â”‚    - For each point:                                         â”‚
â”‚      â€¢ page_content = payload["page_content"]              â”‚
â”‚        (the car description text)                           â”‚
â”‚      â€¢ metadata = all other payload fields                 â”‚
â”‚        (make, model, price, mileage, features...)          â”‚
â”‚                                                             â”‚
â”‚    Output: List of Document objects                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 6: Generation Phase                                   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: llm/backend/rag/chain.py                             â”‚
â”‚ Class: ConversationalRetrievalChain                        â”‚
â”‚                                                             â”‚
â”‚ STEP 6.1: Format Context                                    â”‚
â”‚    Function: format_docs(documents)                         â”‚
â”‚                                                             â”‚
â”‚    Combine retrieved documents into context string:        â”‚
â”‚    """                                                      â”‚
â”‚    --- Car 1 ---                                            â”‚
â”‚    Description: 2024 Tata Nexon. SUV in C-Segment...        â”‚
â”‚    Brand: Tata | Model: Nexon | Price: â‚¹15.50 lakhs...     â”‚
â”‚                                                             â”‚
â”‚    --- Car 2 ---                                            â”‚
â”‚    Description: 2024 Mahindra XUV300. SUV...               â”‚
â”‚    ...                                                      â”‚
â”‚    """                                                      â”‚
â”‚                                                             â”‚
â”‚ STEP 6.2: Format Chat History                                â”‚
â”‚    Function: format_chat_history(history)                   â”‚
â”‚    - Convert last 5 message pairs to text                   â”‚
â”‚    - Format: "Human: ...\nAI: ..."                         â”‚
â”‚                                                             â”‚
â”‚ STEP 6.3: Build Prompt                                      â”‚
â”‚    - Load template: prompts/base_prompt.txt                 â”‚
â”‚    - Fill variables:                                        â”‚
â”‚      â€¢ {context} = formatted car descriptions             â”‚
â”‚      â€¢ {question} = user query                             â”‚
â”‚      â€¢ {chat_history} = formatted history                  â”‚
â”‚                                                             â”‚
â”‚ STEP 6.4: LLM Generation                                     â”‚
â”‚    - Input: Complete prompt with context                    â”‚
â”‚    - Model: Groq (llama-3.3-70b-versatile)                  â”‚
â”‚    - Temperature: 0.4 (factual responses)                  â”‚
â”‚    - Max tokens: 3072                                       â”‚
â”‚                                                             â”‚
â”‚    - LLM generates natural language response:                â”‚
â”‚      "Based on your requirements, here are my top picks:     â”‚
â”‚                                                             â”‚
â”‚      â€¢ Tata Nexon XZ+ at â‚¹15.50 lakhs - Excellent fuel...  â”‚
â”‚      â€¢ Mahindra XUV300 W8 at â‚¹14.20 lakhs - Great value...  â”‚
â”‚      ..."                                                    â”‚
â”‚                                                             â”‚
â”‚ STEP 6.5: Post-Process Answer                                â”‚
â”‚    Function: post_process_answer(raw_answer)                â”‚
â”‚    - Remove markdown artifacts                              â”‚
â”‚    - Clean formatting                                        â”‚
â”‚    - Ensure consistent bullet points                        â”‚
â”‚                                                             â”‚
â”‚ STEP 6.6: Extract Recommendations                           â”‚
â”‚    Function: query_chain()                                  â”‚
â”‚                                                             â”‚
â”‚    From source documents, extract:                          â”‚
â”‚    - De-duplicate by brand+model                            â”‚
â”‚    - Format car info:                                        â”‚
â”‚      {                                                       â”‚
â”‚        id: MongoDB _id,                                     â”‚
â”‚        name: "Tata Nexon",                                  â”‚
â”‚        make: "Tata",                                        â”‚
â”‚        model: "Nexon",                                      â”‚
â”‚        price: 15.50,                                        â”‚
â”‚        mileage: 18.2,                                       â”‚
â”‚        ... (other fields)                                   â”‚
â”‚      }                                                      â”‚
â”‚                                                             â”‚
â”‚    Output:                                                  â”‚
â”‚    {                                                        â”‚
â”‚      answer: "Based on your requirements...",               â”‚
â”‚      recommended: [car1, car2, ...],                        â”‚
â”‚      sources: [doc1_info, doc2_info, ...]                  â”‚
â”‚    }                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 7: Response Refinement                                â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: llm/backend/rag/refiner.py                           â”‚
â”‚                                                             â”‚
â”‚ STEP 7.1: Refine with LLM                                    â”‚
â”‚    Function: refine_response_with_llm()                    â”‚
â”‚                                                             â”‚
â”‚    a) Build refinement prompt:                              â”‚
â”‚       - Original query                                       â”‚
â”‚       - RAG response (from Phase 6)                         â”‚
â”‚       - Recommended cars list                                â”‚
â”‚       - Chat history context                                 â”‚
â”‚                                                             â”‚
â”‚    b) Call LLM (Groq/Gemini):                              â”‚
â”‚       - Model: llama-3.3-70b-versatile                      â”‚
â”‚       - Temperature: 0.5 (more natural language)             â”‚
â”‚       - Purpose: Enhance with general automotive knowledge  â”‚
â”‚                                                             â”‚
â”‚    c) LLM enhances response:                                 â”‚
â”‚       - More conversational tone                            â”‚
â”‚       - Better structure                                     â”‚
â”‚       - Additional context from general knowledge            â”‚
â”‚       - Natural transitions                                  â”‚
â”‚                                                             â”‚
â”‚    Output: Refined answer string                            â”‚
â”‚                                                             â”‚
â”‚ STEP 7.2: Update Chat History                                â”‚
â”‚    - Add (query, refined_answer) to session history          â”‚
â”‚    - Keep only last 10 exchanges                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 8: Response Enrichment (Next.js)                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ File: src/app/api/ai/rag-chat/route.ts                     â”‚
â”‚                                                             â”‚
â”‚ STEP 8.1: Enrich Recommendations                            â”‚
â”‚    For each recommended car:                                â”‚
â”‚                                                             â”‚
â”‚    a) Try fetching by MongoDB ID:                           â”‚
â”‚       GET /api/cars/{id}                                    â”‚
â”‚       - Use car.id from RAG response                        â”‚
â”‚                                                             â”‚
â”‚    b) If fails, search by brand+model:                      â”‚
â”‚       GET /api/cars?search={make} {model}&limit=1           â”‚
â”‚                                                             â”‚
â”‚    c) Enrich with full car data:                            â”‚
â”‚       - Images                                              â”‚
â”‚       - Complete specifications                             â”‚
â”‚       - Consistent format matching frontend Car type        â”‚
â”‚                                                             â”‚
â”‚ STEP 8.2: Format Final Response                             â”‚
â”‚    {                                                        â”‚
â”‚      response: "Refined answer text...",                   â”‚
â”‚      recommendations: [enriched_car1, enriched_car2, ...],  â”‚
â”‚      sources: [...],                                        â”‚
â”‚      metadata: {                                            â”‚
â”‚        sessionId: "...",                                    â”‚
â”‚        timestamp: "...",                                    â”‚
â”‚        backend: "rag",                                      â”‚
â”‚        originalCount: 8,                                   â”‚
â”‚        enrichedCount: 5                                     â”‚
â”‚      }                                                      â”‚
â”‚    }                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 9: Frontend Display                                   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ Component: src/components/features/rag-recommendation-panelâ”‚
â”‚                                                             â”‚
â”‚ 1. Display AI Response                                      â”‚
â”‚    - Show refined answer in chat interface                   â”‚
â”‚    - Format with proper styling                             â”‚
â”‚                                                             â”‚
â”‚ 2. Display Recommendations                                  â”‚
â”‚    - Render car cards in grid                               â”‚
â”‚    - Each card shows:                                        â”‚
â”‚      â€¢ Car image                                            â”‚
â”‚      â€¢ Name (make + model + variant)                       â”‚
â”‚      â€¢ Price in â‚¹ lakhs                                    â”‚
â”‚      â€¢ Mileage in kmpl                                     â”‚
â”‚      â€¢ Key features                                         â”‚
â”‚      â€¢ Action buttons (View, Compare, Favorite)            â”‚
â”‚                                                             â”‚
â”‚ 3. Display Sources (optional)                                â”‚
â”‚    - Show data provenance                                   â”‚
â”‚    - Transparency about data source                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Component Interactions

### Component Dependency Graph

```
main.py (FastAPI)
  â”‚
  â”œâ”€â–º query_understanding.py
  â”‚     â””â”€â–º model.py (get_llm) â”€â”€â–º Groq/Gemini API
  â”‚
  â”œâ”€â–º chain.py (create_chain)
  â”‚     â”œâ”€â–º model.py (get_llm) â”€â”€â–º Groq/Gemini API
  â”‚     â”œâ”€â–º retriever.py (get_retriever)
  â”‚     â”‚     â”œâ”€â–º embed.py (HuggingFaceEmbeddings)
  â”‚     â”‚     â””â”€â–º Qdrant Client â”€â”€â–º Qdrant Vector DB
  â”‚     â””â”€â–º prompts/base_prompt.txt
  â”‚
  â””â”€â–º refiner.py
        â””â”€â–º Groq/Gemini API

retriever.py
  â”œâ”€â–º embed.py (SentenceTransformer)
  â””â”€â–º Qdrant Client

embed.py (Initialization)
  â”œâ”€â–º mongodb_loader.py â”€â”€â–º MongoDB Atlas
  â””â”€â–º Qdrant Client
```

### Data Flow Between Components

```
User Query
    â”‚
    â–¼
[main.py] â”€â”€â–º [query_understanding.py] â”€â”€â–º LLM API
    â”‚                                         â”‚
    â”‚                                         â–¼
    â”‚                                    Understanding JSON
    â”‚                                         â”‚
    â”‚                                         â–¼
    â”œâ”€â–º [chain.py] â”€â”€â–º [retriever.py] â”€â”€â–º Qdrant
    â”‚         â”‚              â”‚                â”‚
    â”‚         â”‚              â”‚                â–¼
    â”‚         â”‚              â””â”€â–º Documents (with metadata)
    â”‚         â”‚                    â”‚
    â”‚         â”‚                    â–¼
    â”‚         â””â”€â–º Format Context â”€â”€â–º LLM API
    â”‚                                    â”‚
    â”‚                                    â–¼
    â”‚                                RAG Response
    â”‚                                    â”‚
    â”‚                                    â–¼
    â””â”€â–º [refiner.py] â”€â”€â–º LLM API â”€â”€â–º Refined Response
                              â”‚
                              â–¼
                        Final Answer + Recommendations
```

---

## ğŸ“Š Data Flow Diagrams

### 1. Initialization Data Flow

```
MongoDB Atlas
    â”‚
    â”‚ (Fetch car documents)
    â–¼
[mongodb_loader.py]
    â”‚
    â”‚ (Transform & create descriptions)
    â–¼
Car Records (with "description" field)
    â”‚
    â”‚ (Generate embeddings)
    â–¼
[embed.py]
    â”‚
    â”‚ (384-dim vectors + metadata)
    â–¼
Qdrant Vector DB
    â”‚
    â””â”€â–º Stored as:
        - Vector: [0.123, -0.456, ...] (384 dims)
        - Payload: {make, model, price, mileage, ...}
        - ID: unique identifier
```

### 2. Query Processing Data Flow

```
User Query: "fuel efficient SUV under 15 lakhs"
    â”‚
    â–¼
[Query Understanding]
    â”‚
    â”œâ”€â–º Filters: {body_type: "SUV", price_max: 15.0, mileage_min: 18.0}
    â””â”€â–º Keywords: "fuel efficient SUV"
         â”‚
         â–¼
    [Embedding Model]
         â”‚
         â–¼
    Query Vector: [0.789, -0.234, ...] (384 dims)
         â”‚
         â–¼
    [Qdrant Search]
         â”‚
         â”œâ”€â–º Vector Similarity (Cosine)
         â””â”€â–º Metadata Filter (body_type=SUV, priceâ‰¤15, mileageâ‰¥18)
              â”‚
              â–¼
    Top 8 Matching Cars
         â”‚
         â”œâ”€â–º page_content: "2024 Tata Nexon. SUV..."
         â””â”€â–º metadata: {make: "Tata", model: "Nexon", price: 15.50, ...}
              â”‚
              â–¼
    [LLM Generation]
         â”‚
         â–¼
    RAG Answer: "Based on your requirements..."
         â”‚
         â–¼
    [Refinement]
         â”‚
         â–¼
    Final Answer + Recommendations
```

---

## ğŸŒ API Endpoints & Request Flow

### Endpoint: `POST /api/ai/rag-chat`

**Request**:
```json
{
  "query": "Show me fuel-efficient SUVs under 15 lakhs",
  "session_id": "session_1234567890" // optional
}
```

**Response**:
```json
{
  "response": "Based on your requirements, here are my top picks:\n\nâ€¢ Tata Nexon...",
  "recommendations": [
    {
      "_id": "507f1f77bcf86cd799439011",
      "brand": "Tata",
      "model": "Nexon",
      "price": 15.50,
      "mileage": 18.2,
      "images": [...],
      ...
    }
  ],
  "sources": [...],
  "metadata": {
    "sessionId": "session_1234567890",
    "timestamp": "2024-01-15T10:30:00Z",
    "backend": "rag",
    "originalCount": 8,
    "enrichedCount": 5
  }
}
```

### Internal FastAPI Endpoint: `POST http://localhost:8000/chat`

**Request**:
```json
{
  "query": "Show me fuel-efficient SUVs under 15 lakhs",
  "session_id": "session_1234567890",
  "filters": null // optional explicit filters
}
```

**Response**:
```json
{
  "answer": "Based on your requirements...",
  "recommended": [
    {
      "id": "507f1f77bcf86cd799439011",
      "name": "Tata Nexon",
      "make": "Tata",
      "model": "Nexon",
      "price": 15.50,
      "mileage": 18.2
    }
  ],
  "sources": [
    {
      "content": "2024 Tata Nexon. SUV in C-Segment...",
      "metadata": {...}
    }
  ]
}
```

---

## âš ï¸ Error Handling & Fallbacks

### Error Handling Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Error: LLM API Failure (Groq/Gemini)                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 1. Try Groq first                                       â”‚
â”‚ 2. If fails â†’ Try Gemini                                â”‚
â”‚ 3. If fails â†’ Try Ollama (local dev)                    â”‚
â”‚ 4. If all fail â†’ Return error with helpful message      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Error: Qdrant Connection Failure                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 1. Check QDRANT_URL in .env                             â”‚
â”‚ 2. Verify Qdrant is running                             â”‚
â”‚ 3. Check network connectivity                            â”‚
â”‚ 4. Return 503 with connection error message             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Error: Missing Qdrant Index                             â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 1. Retry query without filter                           â”‚
â”‚ 2. Log warning about missing index                      â”‚
â”‚ 3. Continue with semantic search only                    â”‚
â”‚ 4. Note: Results may be less precise                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Error: Car Enrichment Failure (Next.js)                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 1. Try fetching by MongoDB ID                           â”‚
â”‚ 2. If fails â†’ Try search by brand+model                â”‚
â”‚ 3. If fails â†’ Return null, filter out in response       â”‚
â”‚ 4. Continue with successfully enriched cars             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Error: Empty Retrieval Results                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 1. LLM generates response acknowledging no matches     â”‚
â”‚ 2. Suggests alternative criteria                        â”‚
â”‚ 3. Uses general automotive knowledge                    â”‚
â”‚ 4. Provides helpful guidance                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Key Configuration Points

### Environment Variables

```bash
# LLM Configuration
GROQ_API_KEY=...              # Primary (fastest)
GEMINI_API_KEY=...            # Fallback
OLLAMA_URL=http://localhost:11434  # Local dev

# Vector Database
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=...            # For Qdrant Cloud
QDRANT_COLLECTION_NAME=cars_rag

# Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
RETRIEVAL_K=8                 # Number of docs to retrieve

# Data Source
MONGODB_URI=...               # MongoDB Atlas connection
MONGODB_DATABASE=autoassist
MONGODB_COLLECTION=cars_new

# Backend
RAG_API_URL=http://localhost:8000  # FastAPI backend URL
```

### Tuning Parameters

- **RETRIEVAL_K**: Number of documents to retrieve (default: 8)
  - Higher = more coverage, but slower
  - Lower = faster, but may miss relevant cars

- **LLM Temperature**: 
  - Query Understanding: 0.3 (structured output)
  - RAG Generation: 0.4 (factual responses)
  - Refinement: 0.5 (natural language)

- **Chat History Length**: Last 10 exchanges (configurable in main.py)

---

## ğŸ“ˆ Performance Characteristics

### Typical Latency Breakdown

```
User Query â†’ Response
â”œâ”€ Query Understanding: ~500ms (LLM call)
â”œâ”€ Retrieval: ~50-100ms (Qdrant search)
â”œâ”€ RAG Generation: ~1-2s (LLM call with context)
â”œâ”€ Refinement: ~500ms (LLM call)
â””â”€ Enrichment: ~200-500ms (MongoDB fetches)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~2-4 seconds (depending on LLM provider)
```

### Optimization Strategies

1. **Session Caching**: Chains cached per session to avoid recreation
2. **Batch Embedding**: Process cars in batches during initialization
3. **Payload Indexes**: Fast metadata filtering in Qdrant
4. **Selective Retrieval**: Only fetch top K most relevant cars
5. **Parallel Enrichment**: Fetch full car data in parallel (Promise.all)

---

## ğŸ¯ Summary

This RAG pipeline combines:

1. **Semantic Search**: Vector similarity for understanding user intent
2. **Structured Filtering**: Metadata filters for precise matching
3. **LLM Understanding**: Deep query analysis with context awareness
4. **Hybrid Retrieval**: Best of both semantic and exact matching
5. **Response Enhancement**: LLM refinement for natural, helpful responses
6. **Conversation Memory**: Context-aware multi-turn conversations

The system is production-ready, scalable, and provides accurate, contextual car recommendations! ğŸš€

